{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Invariant Feature Transform（SIFT）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image scale space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a certain range, no matter whether the object is large or small, the human eye can distinguish it. However, it is difficult for a computer to have the same ability. Therefore, in order for the machine to have a unified understanding of objects at different scales, it is necessary to It is necessary to consider the characteristics of images that exist at different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquisition of scale space is usually achieved using Gaussian blur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian functions with different σ determine the smoothness of the image. A larger σ value corresponds to a blurrier image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-resolution pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference of Gaussian Pyramid (DOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DoG space extreme value detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the extreme point of the scale space, each pixel point is compared with all adjacent points in its image domain (same scale space) and scale domain (adjacent scale space). When it is greater than (or smaller than) all phase points, When it is an adjacent point, the point is the extreme point. As shown in the figure below, the middle detection point needs to be compared with 8 pixels in the 3×3 neighborhood of the image where it is located, and 18 pixels in the 3×3 areas of the adjacent upper and lower layers, for a total of 26 pixels. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precise positioning of key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate boundary response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main direction of feature points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature point can obtain three pieces of information (x, y, σ, θ), namely position, scale and direction. Key points with multiple directions can be copied into multiple copies, and then the direction values are assigned to the copied feature points respectively. One feature point generates multiple feature points with the same coordinates and scales, but different directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate feature description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the gradient calculation of key points, the histogram is used to count the gradient and direction of pixels in the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure the rotation invariance of the feature vector, the feature point should be taken as the center and the coordinate axis should be rotated by an angle θ in the nearby neighborhood, that is, the coordinate axis should be rotated to the main direction of the feature point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take an 8x8 window with the main direction after rotation as the center, and find the gradient amplitude and direction of each pixel. The direction of the arrow represents the gradient direction, and the length represents the gradient amplitude. Then use a Gaussian window to weight it, and finally in each 4x4 Draw a gradient histogram in 8 directions on a small patch, and calculate the cumulative value of each gradient direction to form a seed point, that is, each feature is composed of 4 seed points, and each seed point has 8 directions. vector information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper recommends using 4x4 total 16 seed points to describe each key point, so that a key point will generate a 128-dimensional SIFT feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sift_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opencv SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('test_1.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__ #3.4.1.15 pip install opencv-python==3.4.1.15 pip install opencv-contrib-python==3.4.1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp = sift.detect(gray, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.drawKeypoints(gray, kp, img)\n",
    "\n",
    "cv2.imshow('drawKeypoints', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kp, des = sift.compute(gray, kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6827,)\n"
     ]
    }
   ],
   "source": [
    "print (np.array(kp).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6827, 128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  21.,   8.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 157.,  31.,   3.,   1.,   0.,   0.,\n",
       "         2.,  63.,  75.,   7.,  20.,  35.,  31.,  74.,  23.,  66.,   0.,\n",
       "         0.,   1.,   3.,   4.,   1.,   0.,   0.,  76.,  15.,  13.,  27.,\n",
       "         8.,   1.,   0.,   2., 157., 112.,  50.,  31.,   2.,   0.,   0.,\n",
       "         9.,  49.,  42., 157., 157.,  12.,   4.,   1.,   5.,   1.,  13.,\n",
       "         7.,  12.,  41.,   5.,   0.,   0., 104.,   8.,   5.,  19.,  53.,\n",
       "         5.,   1.,  21., 157.,  55.,  35.,  90.,  22.,   0.,   0.,  18.,\n",
       "         3.,   6.,  68., 157.,  52.,   0.,   0.,   0.,   7.,  34.,  10.,\n",
       "        10.,  11.,   0.,   2.,   6.,  44.,   9.,   4.,   7.,  19.,   5.,\n",
       "        14.,  26.,  37.,  28.,  32.,  92.,  16.,   2.,   3.,   4.,   0.,\n",
       "         0.,   6.,  92.,  23.,   0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
